* TRPO
TRPO是一种经典的强化学习算法，全称是Trust Region Policy Optimization，中文译为信赖域策略优化。
策略梯度算法更新策略时，如何选择合适步长从而确保累积奖励增加是一个关键问题。
TRPO通过限制新策略在旧策略的邻域中搜索，具有
- 累积奖励递增的理论分析，
- 不错的训练效果。

** 优化目标
策略\(\pi\)的累积奖励定义为$J(\pi) = \mathbb{E}_{s_0, a_0, ... \sim \pi} \sum_{t=0}^\infty \gamma^t r(s_t, a_t)$.
Sham Kakade（2012）分析了两个策略\(\tilde{\pi}\)和\(\pi\)之间的累积奖励差值，
\begin{align}
J(\tilde{\pi}) - J(\pi) &= \mathbb{E}_{s_0,a_0, ... \sim \tilde{\pi}} \sum_{t=0}^\infty \gamma^t A_\pi(s_t, a_t) \\
&= \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a).
\end{align}
其中，$A_\pi(s_t, a_t)$表示优势函数，$A_\pi(s_t, a_t) = Q_\pi(s_t, a_t) - V_\pi(s_t)$.


因此，给定当前策略$\pi$，我们可以通过提升差值项来改进模型。
在实际计算过程中，动作分布\(\tilde{\pi}(a|s)\)可以通过重要性采样（importance sampling）解决，
\[ 
\sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s, a),
\]
但状态分布\(\rho_{\tilde{\pi}}(s)\)难以通过重要性采样解决，因为状态分布受决策序列影响，概率依赖很深。
TRPO使用旧策略对应的状态分布\(\rho_{\pi}(s)\)去近似该状态分布。
因此，优化目标转化为最大化下面的近似累积奖励差函数，
\begin{align*}
    L_\pi(\tilde{\pi}) = \sum_s \rho_\pi(s) \sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s,a)
\end{align*}
以上优化目标和普通Actor Critic的优化目标是相同的。可见，普通Actor Critic也有近似优化目标。
TRPO进一步添加了KL散度来约束策略更新，最终的优化目标为，
\begin{align}
\label{eq:trpo:1}
    & \max_{\tilde{\pi}} \sum_s \rho_\pi(s) \sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s,a) \\
    & s.t. ~~~~ \mathbb{E}_{s \sim \rho_\pi} D_{KL}(\pi(\cdot|s) \| \tilde{\pi}(\cdot|s)) \leq \epsilon \nonumber
\end{align}


** 理论分析
优化近似的目标会有两个问题
- 不知道更新方向对不对，
- 不知道如何挑选合适的步长。
TRPO建立了以下的边界分析，
\[
J(\tilde{\pi}) - J(\pi) \geq L_\pi(\tilde{\pi}) - CD_{KL}^\max(\pi, \tilde{\pi}) \\
\text{其中，} C= \frac{4\gamma \epsilon}{(1-\gamma)^2}, \epsilon = \max_{s,a} |A(s,a)|.
\]
以上不等式打通了累积奖励增益$J(\tilde{\pi}) - J(\pi)$和近似目标$L_\pi(\tilde{\pi})$之间的关系。
由此，我们不需要担心上述两个问题，只需优化不等式右边的项。
注意，具体求解优化目标\eqref{eq:trpo:1}时，我们进一步近似了策略约束，将KL散度的最大化操作替换为平均操作。


** 计算过程
求解优化问题\eqref{eq:trpo:1}时，我们对目标进行一阶泰勒近似，得到
\begin{align*}
    \mathbb{E}_{s \sim \rho_{\pi_{\theta_{old}}}(\cdot), a \sim \pi_{\theta_{old}}(\cdot|s)} \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A_{\pi_{\theta_{old}}}(s,a)
    = g^\top (\theta - \theta_{old}) + K_0,
\end{align*}
其中\(g\)表示\(A_{\pi_{\theta_{old}}}(s,a) \pi_\theta(a|s) / \pi_{\theta_{old}}(a|s)\)在\(\theta = \theta_{old}\)处导数的期望，
\(K_0\)表示和\(\theta\)无关的常数。
我们对策略约束使用二阶泰勒近似，可以得到
\begin{align*}
    \mathbb{E}_{s \sim \rho_{\pi_{\theta_{old}}}(\cdot)} D_\alpha (\pi_{\theta_{old}}(\cdot|s) \| \pi_\theta(\cdot|s)) 
    = \frac{1}{2} (\theta - \theta_{old})^\top H (\theta - \theta_{old}) + K_1,
\end{align*}
其中\(H\)表示在等式左边项在\(\theta=\theta_{old}\)处的二阶导数，\(K_1\)表示和\(\theta\)无关的常数。
注意，上面等式的右边没有一阶项，这是因为左边项在\(\theta = \theta_{old}\)的一阶项为零。
在实现过程中，上述一阶导数和二阶导数期望的计算都是使用采样的数据近似计算得到的。


我们去掉与\(\theta\)无关的常数项之后，可以得到如下的优化问题，
\begin{align*}
        & \min_\theta ~   - g^\top (\theta - \theta_{old}) \\
        & s.t. ~~ \frac{1}{2}(\theta - \theta_{old})^\top H (\theta - \theta_{old}) \leq \epsilon.
\end{align*}
上式可以转化成等价的最小最大问题，
\begin{align*}
    \min_\theta  \max_{\lambda \geq 0} ~  L(\theta, \lambda) = - g^\top(\theta - \theta_{old}) + 
    \lambda \cdot  (\frac{1}{2} (\theta - \theta_{old})^\top H (\theta - \theta_{old}) - \epsilon). 
\end{align*}
接下来我们使用KKT条件求解上述问题。
根据\(L(\theta, \lambda)\)的稳定性，我们可以得到\(\partial L/\partial \theta = 0\)，
进而推导出\(\theta = \theta_{old} + \lambda^{-1} H^{-1}g\).
然后我们代入\(\theta_{old} + \lambda^{-1} H^{-1}g\) for \(\theta\)，
可以计算得到$\lambda = \sqrt{ (g^\top H^{-1} g)/(2\epsilon) }$.
从而可以计算得出问题的解$\theta = \theta_{old} + \sqrt{ 2\epsilon (g^\top H^{-1}g)^{-1} } H^{-1}g$.


* PPO
PPO全称是Proximal Policy Optimization，中文译为近端策略优化。
PPO简化了TRPO中复杂的计算流程，从而降低了计算复杂度以及实现难度。


** 优化目标
PPO简化了TRPO中的优化问题，将优化问题转化为，
\[
\max_\theta \mathbb{E}_{s \sim \rho_{\pi_{\theta_{old}}}(\cdot), a \sim \pi_{\theta_{old}}(\cdot|s)} \min \left(  \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A_{\pi_{\theta_{old}}}(s,a), clip \left(\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} , 1-\epsilon, 1+\epsilon \right) A_{\pi_{\theta_{old}}}(s,a)\right)
\]
沿用TRPO中的思路，将新策略约束在旧策略的邻域内：首先使用clip操作，约束新旧策略在动作概率上的比率，获得一个近似目标；
然后通过min操作，确保最终的优化目标是一个真实目标的下界。
最后，求解优化问题来抬高下界，从而达到改进目标的效果。


** 直观释义
优势函数的定义是$A_{\pi_{old}}(s,a) = Q_{\pi_{old}}(s,a) - V_{\pi_{old}}(s)$，
表示采样动作相对于平均动作的优势值。

- 当$A > 0$时，表示此时优势值为正，即当前策略在该状态上正确执行，没必要在此样本上过度修正算法。
  因此，min操作和clip操作组合使得如果比值超过$1+\epsilon$，最终为$1+\epsilon$，否则保持原值。
  这样就限制了更新程度。
- 当$A < 0$时，表示此时优势值为负（$-\max(r, clip(r, 1-\epsilon, 1+\epsilon))A$，$r$表示比值），即当前策略在该状态上效果不好，有必要在此样本上修正算法。
  因此，min操作和clip操作组合使得如果比值低于$1-\epsilon$，最后为$1-\epsilon$，否则保持原值。
  这样就使得更新成都可以很大。

直观释义的出发点：比值的大小会影响梯度的大小，因为近似函数可以近似理解成多项式函数。

** 改良方案

*** 更改clip和min操作
- 当$A > 0$时，比值超过$1+\epsilon$后，保持不变。和原来一致。
- 当$A < 0$时，比值始终保持不变，鼓励在负值时更新算法。

*** clip优势函数A
- 当$A > 0$时，保持不变
- 当$A < 0$时，clip A到一个合适的区间。

*** 提高A值准确度
使用TD3里面的技巧



** 代码实现
*** 统一离散和连续情况
- 对于状态而言，离散和连续没区别，最后统一输出状态值。且大部分是连续的。
- 对于动作而言，离散输出N个动作；连续使用Gaussian，输出均值，共享方差。



* SAC


* TD3 

分层rl分别考虑状态和动作
简单事情不确定时，遍历解决。
