

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>TRPO &mdash; rlpack 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="PPO" href="ppo.html" />
    <link rel="prev" title="A2C" href="a2c.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> rlpack
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html#usage">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption"><span class="caption-text">Algorithms</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="a2c.html">A2C</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TRPO</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">优化目标</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id4">理论分析</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">计算过程</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">参考文献</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">PPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">DDPG</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/rlpack.algos.html">rlpack.algos package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/rlpack.environment.html">rlpack.environment package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">rlpack</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">&lt;no title&gt;</a> &raquo;</li>
        
      <li>TRPO</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algos/trpo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="trpo">
<h1>TRPO<a class="headerlink" href="#trpo" title="Permalink to this headline">¶</a></h1>
<p>TRPO是一种经典的强化学习算法，全称是Trust Region Policy Optimization，中文译为信赖域策略优化。 策略梯度算法更新策略时，如何选择合适步长从而确保累积奖励增加是一个关键问题。
TRPO通过限制新策略在旧策略的邻域中搜索，具有</p>
<ul class="simple">
<li>累积奖励递增的理论分析，</li>
<li>不错的训练效果。</li>
</ul>
<div class="section" id="id1">
<h2>优化目标<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>策略 <span class="math notranslate nohighlight">\(\pi\)</span> 的累积奖励定义为 <span class="math notranslate nohighlight">\(J(\pi) = \mathbb{E}_{s_0, a_0, ... \sim \pi} \sum_{t=0}^\infty \gamma^t r(s_t, a_t)\)</span>.
Sham Kakade（2012）分析了两个策略 <span class="math notranslate nohighlight">\(\tilde{\pi}\)</span> 和 <span class="math notranslate nohighlight">\(\pi\)</span> 之间的累积奖励差值，</p>
<div class="math notranslate nohighlight">
\[\begin{split}J(\tilde{\pi}) - J(\pi) &amp;= \mathbb{E}_{s_0,a_0, ... \sim \tilde{\pi}} \sum_{t=0}^\infty \gamma^t A_\pi(s_t, a_t) \\
&amp;= \sum_s \rho_{\tilde{\pi}}(s) \sum_a \tilde{\pi}(a|s) A_\pi(s, a).\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(A_\pi(s_t, a_t)\)</span> 表示优势函数，<span class="math notranslate nohighlight">\(A_\pi(s_t, a_t) = Q_\pi(s_t, a_t) - V_\pi(s_t)\)</span>.</p>
<p>因此，给定当前策略 <span class="math notranslate nohighlight">\(\pi\)</span> ，我们可以通过提升差值项来改进模型。
在实际计算过程中，动作分布 :math:<a href="#id2"><span class="problematic" id="id3">`</span></a>tilde{pi}(a|s)`可以通过重要性采样（importance sampling）解决，</p>
<div class="math notranslate nohighlight">
\[\sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s, a),\]</div>
<p>但状态分布 <span class="math notranslate nohighlight">\(\rho_{\tilde{\pi}}(s)\)</span> 难以通过重要性采样解决，因为状态分布受决策序列影响，概率依赖很深。
TRPO使用旧策略对应的状态分布 <span class="math notranslate nohighlight">\(\rho_{\pi}(s)\)</span> 去近似该状态分布。
因此，优化目标转化为最大化下面的近似累积奖励差函数，</p>
<div class="math notranslate nohighlight">
\[L_\pi(\tilde{\pi}) = \sum_s \rho_\pi(s) \sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s,a)\]</div>
<p>以上优化目标和普通Actor Critic的优化目标是相同的。可见，普通Actor Critic也有近似优化目标。
TRPO进一步添加了KL散度来约束策略更新，最终的优化目标为，</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp; \max_{\tilde{\pi}} \sum_s \rho_\pi(s) \sum_a \pi(a|s) \frac{\tilde{\pi}(a|s)}{\pi(a|s)} A_\pi(s,a) \\
&amp; s.t. ~~~~ \mathbb{E}_{s \sim \rho_\pi} D_{KL}(\pi(\cdot|s) \| \tilde{\pi}(\cdot|s)) \leq \epsilon \nonumber\end{split}\]</div>
</div>
<div class="section" id="id4">
<h2>理论分析<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>优化近似的目标会有两个问题</p>
<ul class="simple">
<li>不知道更新方向对不对，</li>
<li>不知道如何挑选合适的步长。</li>
</ul>
<p>TRPO建立了以下的边界分析，</p>
<div class="math notranslate nohighlight">
\[\begin{split}J(\tilde{\pi}) - J(\pi) \geq L_\pi(\tilde{\pi}) - CD_{KL}^\max(\pi, \tilde{\pi}) \\
\text{其中，} C= \frac{4\gamma \epsilon}{(1-\gamma)^2}, \epsilon = \max_{s,a} |A(s,a)|.\end{split}\]</div>
<p>以上不等式打通了累积奖励增益 <span class="math notranslate nohighlight">\(J(\tilde{\pi}) - J(\pi)\)</span> 和近似目标 <span class="math notranslate nohighlight">\(L_\pi(\tilde{\pi})\)</span> 之间的关系。
由此，我们不需要担心上述两个问题，只需优化不等式右边的项。
注意，具体求解优化目标时，我们进一步近似了策略约束，将KL散度的最大化操作替换为平均操作。</p>
</div>
<div class="section" id="id5">
<h2>计算过程<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>求解优化问题时，我们对目标进行一阶泰勒近似，得到</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{s \sim \rho_{\pi_{\theta_{old}}}(\cdot), a \sim \pi_{\theta_{old}}(\cdot|s)} \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A_{\pi_{\theta_{old}}}(s,a)
= g^\top (\theta - \theta_{old}) + K_0,\]</div>
<p>其中 :math`g` 表示 <span class="math notranslate nohighlight">\(A_{\pi_{\theta_{old}}}(s,a) \pi_\theta(a|s) / \pi_{\theta_{old}}(a|s)\)</span> 在 <span class="math notranslate nohighlight">\(\theta = \theta_{old}\)</span> 处导数的期望，
<span class="math notranslate nohighlight">\(K_0\)</span> 表示和 <span class="math notranslate nohighlight">\(\theta\)</span> 无关的常数。
我们对策略约束使用二阶泰勒近似，可以得到</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{s \sim \rho_{\pi_{\theta_{old}}}(\cdot)} D_\alpha (\pi_{\theta_{old}}(\cdot|s) \| \pi_\theta(\cdot|s))
= \frac{1}{2} (\theta - \theta_{old})^\top H (\theta - \theta_{old}) + K_1,\]</div>
<p>其中 <span class="math notranslate nohighlight">\(H\)</span> 表示在等式左边项在 <span class="math notranslate nohighlight">\(\theta=\theta_{old}\)</span> 处的二阶导数，<span class="math notranslate nohighlight">\(K_1\)</span> 表示和 <span class="math notranslate nohighlight">\(\theta\)</span> 无关的常数。
注意，上面等式的右边没有一阶项，这是因为左边项在 <span class="math notranslate nohighlight">\(\theta = \theta_{old}\)</span> 的一阶项为零。
在实现过程中，上述一阶导数和二阶导数期望的计算都是使用采样的数据近似计算得到的。</p>
<p>我们去掉与 <span class="math notranslate nohighlight">\(\theta\)</span> 无关的常数项之后，可以得到如下的优化问题，</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp; \min_\theta ~   - g^\top (\theta - \theta_{old}) \\
&amp; s.t. ~~ \frac{1}{2}(\theta - \theta_{old})^\top H (\theta - \theta_{old}) \leq \epsilon.\end{split}\]</div>
<p>上式可以转化成等价的最小最大问题，</p>
<div class="math notranslate nohighlight">
\[\min_\theta  \max_{\lambda \geq 0} ~  L(\theta, \lambda) = - g^\top(\theta - \theta_{old}) +
\lambda \cdot  (\frac{1}{2} (\theta - \theta_{old})^\top H (\theta - \theta_{old}) - \epsilon).\]</div>
<p>接下来我们使用KKT条件求解上述问题。
根据 <span class="math notranslate nohighlight">\(L(\theta, \lambda)\)</span> 的稳定性，我们可以得到 <span class="math notranslate nohighlight">\(\partial L/\partial \theta = 0\)</span>，
进而推导出 <span class="math notranslate nohighlight">\(\theta = \theta_{old} + \lambda^{-1} H^{-1}g\)</span>.
然后我们将其带入到  <span class="math notranslate nohighlight">\(\partial L/\partial \lambda = 0\)</span> ，
可以计算得到 <span class="math notranslate nohighlight">\(\lambda = \sqrt{ (g^\top H^{-1} g)/(2\epsilon) }\)</span>.
从而可以计算得出问题的解 <span class="math notranslate nohighlight">\(\theta = \theta_{old} + \sqrt{ 2\epsilon (g^\top H^{-1}g)^{-1} } H^{-1}g\)</span>.</p>
</div>
<div class="section" id="id6">
<h2>参考文献<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>[1] Schulman, John, et al. “Trust region policy optimization.” International Conference on Machine Learning. 2015.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ppo.html" class="btn btn-neutral float-right" title="PPO" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="a2c.html" class="btn btn-neutral" title="A2C" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, x

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>